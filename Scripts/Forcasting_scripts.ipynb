{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# **2 Prediction Pipeline**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **2.1 Library**"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T10:34:11.437865Z",
     "start_time": "2025-10-27T10:34:11.389325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import time\n",
    "import pyspark.sql.functions as F\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T10:43:51.903939Z",
     "start_time": "2025-10-27T10:43:51.877651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from kafka3 import KafkaAdminClient\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession,DataFrame,Window,window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.ml import Pipeline,PipelineModel,Transformer,Estimator,Model\n",
    "from pyspark.ml.feature import Imputer,Bucketizer\n",
    "from pyspark.ml.param.shared import HasInputCols,HasOutputCol\n",
    "from pyspark.ml.evaluation import Evaluator\n",
    "from pyspark.ml.util import DefaultParamsReadable,DefaultParamsWritable"
   ],
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **2.2 Session Creation**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ----------------------------------------------------- Create system variable\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = (\n",
    "    '--packages org.apache.spark:spark-streaming-kafka-'\n",
    "    '0-10_2.12:3.5.0,org.apache.spark:spark-sql-kafka-'\n",
    "    '0-10_2.12:3.5.0 pyspark-shell'\n",
    ")\n",
    "## ----------------------------------------------------- Params definition\n",
    "topic = \"stream_weather_data\"\n",
    "host_ip = \"kafka\"\n",
    "master = \"local[4]\"\n",
    "app_name = \"data_transformation_pipeline\"\n",
    "## ----------------------------------------------------- Configuration\n",
    "config = (\n",
    "    SparkConf()\n",
    "    .setMaster(value=master)    # Set master and number of cores to use\n",
    "    .setAppName(value=app_name) # Set the application name\n",
    "    .set(\"spark.sql.session.timeZone\", \"Australia/Melbourne\") # Set the time zone\n",
    "    .set(\"spark.executor.memory\", \"10g\")\n",
    "    .set(\"spark.driver.memory\", \"10g\")\n",
    ")\n",
    "## ----------------------------------------------------- Creation\n",
    "spark_session = SparkSession.builder.config(conf = config).getOrCreate()\n",
    "## ----------------------------------------------------- Stream DataFrame\n",
    "DF_weather_stream = (\n",
    "    spark_session\n",
    "    .readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", f\"{host_ip}:9092\")\n",
    "    .option(\"subscribe\", topic)\n",
    "    .option(\"startingOffsets\", \"latest\")\n",
    "    .option(\"maxOffsetsPerTrigger\", 336)\n",
    "    .load()\n",
    ")\n",
    "DF_weather_stream.printSchema()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **2.3 Schema Definition**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ----------------------------------------------------- Building Information\n",
    "# Metadata:\n",
    "# -------------------\n",
    "# site_id     Integer\n",
    "# building_id Integer\n",
    "# primary_use String\n",
    "# square_feet Integer\n",
    "# floor_count Integer\n",
    "# row_id      Integer\n",
    "# year_built  Integer\n",
    "# latent_y    Decimal\n",
    "# latent_s    Decimal\n",
    "# latent_r    Decimal\n",
    "# -------------------\n",
    "data_schema_build = StructType([\n",
    "    StructField(\"site_id\",     IntegerType()),\n",
    "    StructField(\"building_id\", IntegerType()),\n",
    "    StructField(\"primary_use\", StringType()),\n",
    "    StructField(\"square_feet\", IntegerType()),\n",
    "    StructField(\"floor_count\", IntegerType()),\n",
    "    StructField(\"row_id\",      IntegerType()),\n",
    "    StructField(\"year_built\",  IntegerType()),\n",
    "    StructField(\"latent_y\",    DecimalType()),\n",
    "    StructField(\"latent_s\",    DecimalType()),\n",
    "    StructField(\"latent_r\",    DecimalType())\n",
    "])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **2.4 Streaming Query and Information Loading**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **2.4.1 Weather Stream Definition**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ----------------------------------------------------- Weather Schema\n",
    "# Explanation:\n",
    "#   We send the data in the format:\n",
    "#       {...:...}{...:...}{...:...}\n",
    "#   Which can be represent by:\n",
    "#           StructType(\n",
    "#               StructField(name, type)\n",
    "#           )\n",
    "data_schema_weather = StructType([\n",
    "        StructField(\"site_id\",            StringType()),\n",
    "        StructField(\"timestamp\",          StringType()),\n",
    "        StructField(\"air_temperature\",    StringType()),\n",
    "        StructField(\"cloud_coverage\",     StringType()),\n",
    "        StructField(\"dew_temperature\",    StringType()),\n",
    "        StructField(\"sea_level_pressure\", StringType()),\n",
    "        StructField(\"wind_direction\",     StringType()),\n",
    "        StructField(\"wind_speed\",         StringType()),\n",
    "        StructField(\"weather_ts\",         IntegerType())\n",
    "])\n",
    "DF_weather_stream = (\n",
    "    ## ------------------------------------------------- Extract DataFrame from Value\n",
    "    DF_weather_stream\n",
    "    .select(\n",
    "        F.from_json(\n",
    "            col = F.col(\"value\").cast(\"string\"),\n",
    "            schema  = data_schema_weather,\n",
    "            # I use FAILFAST mode to prevent hidden the error,\n",
    "            # since we are only allow one type of input,\n",
    "            # if there are multi type input type,\n",
    "            # use mode = PERMISSIVE\n",
    "            options = {\"mode\":\"FAILFAST\"}\n",
    "        ).alias(\"array_value\")\n",
    "    )\n",
    "    ## ------------------------------------------------- Expanding\n",
    "    .select(\n",
    "        F\n",
    "        # The function of pyspark.functions.inline will will do two things:\n",
    "        #   1. Explode element in ArrayType into multi lines\n",
    "        #   2. Expand key - value pair into multi columns\n",
    "        .inline(\"array_value\")\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"site_id\").cast(IntegerType()).alias(\"site_id\"),\n",
    "        F.col(\"timestamp\").cast(TimestampType()).alias(\"timestamp\"),\n",
    "        F.col(\"air_temperature\").cast(DecimalType()).alias(\"air_temperature\"),\n",
    "        F.col(\"cloud_coverage\").cast(DecimalType()).alias(\"cloud_coverage\"),\n",
    "        F.col(\"dew_temperature\").cast(DecimalType()).alias(\"dew_temperature\"),\n",
    "        F.col(\"sea_level_pressure\").cast(DecimalType()).alias(\"sea_level_pressure\"),\n",
    "        F.col(\"wind_direction\").cast(DecimalType()).alias(\"wind_direction\"),\n",
    "        F.col(\"wind_speed\").cast(DecimalType()).alias(\"wind_speed\"),\n",
    "        F.col(\"weather_ts\").cast(TimestampType()).alias(\"weather_ts\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **2.4.2 Building Metadata**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ----------------------------------------------------- Params Definition\n",
    "file_path = \"../DataStorage/BuildingMetadata/\"\n",
    "file_name = \"new_building_information.csv\"\n",
    "## ----------------------------------------------------- DataFrame Read In\n",
    "DF_building = spark_session.read.csv(\n",
    "    path   = file_path + file_name,\n",
    "    schema = data_schema_build,\n",
    "    header = True\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## **2.5 Watermark**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DF_weather_stream = (\n",
    "    DF_weather_stream\n",
    "    # For my understanding,\n",
    "    # since there could be some late message,\n",
    "    # if without watermark,\n",
    "    # spark stream may process the corresponding batch over and over again,\n",
    "    # this will\n",
    "    #   1. Increase the workload\n",
    "    #   2. Effect the result\n",
    "    .withWatermark(eventTime=\"weather_ts\", delayThreshold=\"5 seconds\") # 5 seconds as threshold\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **2.6 Transformation Pipeline**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **2.6.1 Weather Pipeline**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **2.6.1.1 Static Data**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ============================================================================================= Implementation\n",
    "## --------------------------------------------------------------------------------------------- Reading Data\n",
    "filepath = \"../dataset/\"\n",
    "filename = \"weather.csv\"\n",
    "\n",
    "schema_weather = StructType([\n",
    "    StructField(\"site_id\",            IntegerType()),\n",
    "    StructField(\"timestamp\",          TimestampType()),\n",
    "    StructField(\"air_temperature\",    DecimalType()),\n",
    "    StructField(\"cloud_coverage\",     DecimalType()),\n",
    "    StructField(\"dew_temperature\",    DecimalType()),\n",
    "    StructField(\"sea_level_pressure\", DecimalType()),\n",
    "    StructField(\"wind_direction\",     DecimalType()),\n",
    "    StructField(\"wind_speed\",         DecimalType())\n",
    "])\n",
    "\n",
    "DF_weather = spark_session.read.csv(filepath+filename,schema=schema_weather, header=True)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **2.6.1.2 Imputation**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ============================================================================================= Implementation\n",
    "## --------------------------------------------------------------------------------------------- Params Definition\n",
    "target_table = DF_weather\n",
    "imputation_strategy = \"mean\"\n",
    "input_cols_impute  = list(set(DF_weather.columns) - {\"site_id\", \"timestamp\", \"weather_ts\"})\n",
    "output_cols_impute = [f\"{_name}_impute\" for _name in input_cols_impute]\n",
    "## ============================================================================================= Pipeline List Update\n",
    "imputer = Imputer(\n",
    "    strategy     = imputation_strategy,\n",
    "    inputCols    = input_cols_impute,\n",
    "    outputCols   = output_cols_impute\n",
    ")\n",
    "pipeline_list01 = [imputer]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **2.6.1.3 Time Aggregation**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ============================================================================================= Explanation\n",
    "# About Transformer:\n",
    "#   This is the meta abstract class for every transformer,\n",
    "#   it requires every new transformer class to implement method _transform.\n",
    "#   I learn this from:\n",
    "#       https://github.com/apache/spark/blob/master/python/pyspark/ml/base.py\n",
    "#   And also, the father class for Transformer is Params,\n",
    "#   which implement the method self._set,\n",
    "#   return of it is the class itself,\n",
    "#   Refer to:\n",
    "#       https://github.com/apache/spark/blob/master/python/pyspark/ml/param/__init__.py\n",
    "# About DefaultParamsReadable and DefaultParamsWritable:\n",
    "#   To save our pipeline model,\n",
    "#   we need to herit these two classes,\n",
    "#   Refer to:\n",
    "#       DefaultParamsReadable:\n",
    "#       https://archive.apache.org/dist/spark/docs/3.5.0/api/python/_modules/pyspark/ml/util.html#DefaultParamsReadable\n",
    "#       DefaultParamsRWritable:\n",
    "#       https://archive.apache.org/dist/spark/docs/3.5.0/api/python/_modules/pyspark/ml/util.html#DefaultParamsWritable\n",
    "## ============================================================================================= Implementation\n",
    "class TimeAggregator(\n",
    "    Transformer,\n",
    "    DefaultParamsReadable,\n",
    "    DefaultParamsWritable\n",
    "):\n",
    "\n",
    "    @staticmethod\n",
    "    def _transform(\n",
    "            dataset:DataFrame\n",
    "    ) -> DataFrame:\n",
    "        \"\"\" Initialize the transformer \"\"\"\n",
    "        _tmp = (\n",
    "            dataset\n",
    "            ## --------------------------------------------------------------------------------- Time Extraction\n",
    "            .withColumns({\n",
    "                \"year\" : F.year(F.col(\"timestamp\")).cast(\"integer\"),\n",
    "                \"month\": F.month(F.col(\"timestamp\")).cast(\"integer\"),\n",
    "                \"day\"  : F.day(F.col(\"timestamp\")).cast(\"integer\"),\n",
    "                \"hour\" : F.hour(F.col(\"timestamp\")).cast(\"integer\")\n",
    "            })\n",
    "            ## --------------------------------------------------------------------------------- Binning\n",
    "            .withColumn(\n",
    "                colName = \"start_hour\",\n",
    "                col     = F.when(condition = (F.col(\"hour\") >=  0) & (F.col(\"hour\") <   6), value =  0).\n",
    "                            when(condition = (F.col(\"hour\") >=  6) & (F.col(\"hour\") <  12), value =  6).\n",
    "                            when(condition = (F.col(\"hour\") >= 12) & (F.col(\"hour\") <  18), value = 12).\n",
    "                            when(condition = (F.col(\"hour\") >= 18) , value = 18).\n",
    "                            otherwise(value = None)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if \"weather_ts\" in _tmp.columns:\n",
    "            _tmp = (\n",
    "                _tmp\n",
    "                ## --------------------------------------------------------------------------------- Aggregation\n",
    "                .groupBy(F.col(\"site_id\"),F.col(\"year\"),\n",
    "                         F.col(\"month\"),F.col(\"day\"), F.col(\"start_hour\"),\n",
    "                         F.window(F.col(\"weather_ts\"), \"7 seconds\"))\n",
    "            )\n",
    "        else:\n",
    "            _tmp = (\n",
    "                _tmp\n",
    "                ## --------------------------------------------------------------------------------- Aggregation\n",
    "                .groupBy(F.col(\"site_id\"), F.col(\"year\"),\n",
    "                         F.col(\"month\"), F.col(\"day\"), F.col(\"start_hour\"))\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            _tmp\n",
    "            .agg(\n",
    "                F.mean(F.col(\"air_temperature_impute\")).alias(\"avg_air_temperature_impute\"),\n",
    "                F.mean(F.col(\"cloud_coverage_impute\")).alias(\"avg_cloud_coverage_impute\"),\n",
    "                F.mean(F.col(\"dew_temperature_impute\")).alias(\"avg_dew_temperature_impute\"),\n",
    "                F.mean(F.col(\"sea_level_pressure_impute\")).alias(\"avg_sea_level_pressure_impute\"),\n",
    "                F.mean(F.col(\"wind_direction_impute\")).alias(\"avg_wind_direction_impute\"),\n",
    "                F.mean(F.col(\"wind_speed_impute\")).alias(\"avg_wind_speed_impute\"),\n",
    "            )\n",
    "        )\n",
    "## ============================================================================================= Pipeline List Update\n",
    "time_aggregator = TimeAggregator()\n",
    "pipeline_list02 = pipeline_list01 + [time_aggregator]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **2.6.1.4 Quantile Extraction**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ============================================================================================= Implementation\n",
    "def quantile_bucket_splits(\n",
    "        targetDF:DataFrame,\n",
    "        targetCol:str,\n",
    "        bucketRange:tuple,\n",
    "        quantileList:list=None\n",
    ") -> list:\n",
    "    \"\"\"\n",
    "    Get the quantile bucket splits.\n",
    "    :param targetDF: Target DataFrame (type:pyspark.sql.DataFrame)\n",
    "    :param targetCol: Target feature (type:str)\n",
    "    :param bucketRange: The bucket value range, e.g., greater than 35 -> (35,None) (type:tuple)\n",
    "    :param quantileList: The quantile list, default is 5 equal quantile bucketing (type:list)\n",
    "    :return: The bucket break point list (type:list)\n",
    "    \"\"\"\n",
    "\n",
    "    if quantileList is None and quantileList != []:\n",
    "        quantileList = [.2,.4,.6,.8]\n",
    "\n",
    "    targetFeature = targetDF.select(F.col(targetCol).cast(\"double\").alias(targetCol))\n",
    "\n",
    "    ## ----------------------------------------------------------------------------------------- Data Splitting\n",
    "    if bucketRange[0] is None and bucketRange[1] is not None:\n",
    "        # Pattern like [None,breakPoint]\n",
    "        breakPoint = bucketRange[1]\n",
    "        targetRange = targetFeature.filter(F.col(targetCol) <= breakPoint)\n",
    "        remainRange = targetFeature.filter(F.col(targetCol) >  breakPoint)\n",
    "    elif bucketRange[1] is None and bucketRange[0] is not None:\n",
    "        # Pattern like [breakPoint,None]\n",
    "        breakPoint = bucketRange[0]\n",
    "        targetRange = targetFeature.filter(F.col(targetCol) >= breakPoint)\n",
    "        remainRange = targetFeature.filter(F.col(targetCol) <  breakPoint)\n",
    "    else:\n",
    "        raise ValueError(\"The range is invalid, please check your range.\")\n",
    "\n",
    "    ## ----------------------------------------------------------------------------------------- Quantile Part\n",
    "    if quantileList != []:\n",
    "        # Which means bucket by quantile\n",
    "        quantileBucket = list(targetRange\\\n",
    "            .toPandas()\\\n",
    "            .quantile(quantileList)\\\n",
    "            .iloc[:,0]) # Get the point list\n",
    "    else:\n",
    "        # Which means bucket by specific value\n",
    "        quantileBucket = [breakPoint]\n",
    "\n",
    "    ## ----------------------------------------------------------------------------------------- Normal Part\n",
    "    # The normal part\n",
    "    normalBucket = list(remainRange\\\n",
    "        .distinct()\\\n",
    "        .toPandas()\\\n",
    "        .iloc[:,0])\n",
    "\n",
    "    bucketSplits = [float(_element) for _element in normalBucket + quantileBucket]\n",
    "    bucketSplits.sort()\n",
    "\n",
    "    return [-float(\"inf\")] + bucketSplits + [float(\"inf\")]\n",
    "\n",
    "## ============================================================================================= Extraction\n",
    "## --------------------------------------------------------------------------------------------- Parameter Definition\n",
    "target_col = \"avg_cloud_coverage_impute\"\n",
    "bucket_range = (5,None)\n",
    "quantile_list = [0.5]\n",
    "## --------------------------------------------------------------------------------------------- Cleaning\n",
    "tmp_pipeline = Pipeline(stages = pipeline_list01 + [TimeAggregator()])\n",
    "tmp_DF = tmp_pipeline.fit(dataset = DF_weather).transform(dataset = DF_weather)\n",
    "## --------------------------------------------------------------------------------------------- Extracting\n",
    "quantile_cloud_coverage = quantile_bucket_splits(\n",
    "    targetDF     = tmp_DF,\n",
    "    targetCol    = target_col,\n",
    "    bucketRange  = bucket_range,\n",
    "    quantileList = quantile_list\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **2.6.1.5 Peak Month Assigning**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ============================================================================================= Explanation\n",
    "## --------------------------------------------------------------------------------------------- Resource\n",
    "# Why I define two classes:\n",
    "#   I want to use pyspark.ml.Pipeline,\n",
    "#   when calling fit, it will call ._fit function for estimator\n",
    "#   and ._transform for transformer,\n",
    "#   therefore, I need to define two class for different stages\n",
    "#   Refer to:\n",
    "#       https://archive.apache.org/dist/spark/docs/3.5.0/api/python/_modules/pyspark/ml/pipeline.html#Pipeline\n",
    "#   To be specific, Model class just another transformer class,\n",
    "#   Refer to:\n",
    "#       https://archive.apache.org/dist/spark/docs/3.5.0/api/python/_modules/pyspark/ml/base.html#Model\n",
    "## --------------------------------------------------------------------------------------------- Design\n",
    "# I first group the data by site_id and month,\n",
    "# then calculate the mean of the air_temperature in each group.\n",
    "# Then I define two pyspark.sql.Window,\n",
    "# all partitioning the data by site_id,\n",
    "# and sort the data in descending and ascending order.\n",
    "# Now, the index for each month is the rank for the highest and lowest month.\n",
    "# Finally, assign True to is_peak of those month that are the top 5 rank of highest or lowest month,\n",
    "# and False to the other.\n",
    "## ============================================================================================= Implementation\n",
    "class PeakMonthExtractorModel(\n",
    "    Model,\n",
    "    DefaultParamsReadable,\n",
    "    DefaultParamsWritable\n",
    "):\n",
    "    def __init__(self,isPeakDF:DataFrame):\n",
    "        super().__init__()\n",
    "        self._is_peak_DF = isPeakDF\n",
    "\n",
    "    def _transform(\n",
    "            self,\n",
    "            dataset:DataFrame\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Mark the peak month in DF.\n",
    "        \"\"\"\n",
    "        ## ------------------------------------------------------------------------------------- Assigning\n",
    "        return self._is_peak_DF.join(\n",
    "            other = dataset,\n",
    "            on    = [\"site_id\",\"month\"],\n",
    "            how   = \"inner\"\n",
    "        )\n",
    "\n",
    "class PeakMonthExtractor(\n",
    "    Estimator,\n",
    "    DefaultParamsReadable,\n",
    "    DefaultParamsWritable\n",
    "):\n",
    "    @staticmethod\n",
    "    def _fit(\n",
    "            dataset:DataFrame\n",
    "    ) -> DataFrame:\n",
    "        \"\"\"\n",
    "        Mark the peak month in DF.\n",
    "        :param DF: target table (type:spark.sql.DataFrame)\n",
    "        :return: result (type:spark.sql.DataFrame)\n",
    "        \"\"\"\n",
    "        ## ------------------------------------------------------------------------------------- Window Definition\n",
    "        window_asce = Window.partitionBy([\"site_id\"]).orderBy(\"temp\")         # Highest month  ascend order\n",
    "        window_desc = Window.partitionBy([\"site_id\"]).orderBy(F.desc(\"temp\")) # Lowest month:  descend order\n",
    "\n",
    "        ## ------------------------------------------------------------------------------------- Peak Month Extraction\n",
    "        _is_peak_DF = (\n",
    "            dataset.\n",
    "            select([\"site_id\",\"month\",\"avg_air_temperature_impute\"]).\n",
    "            groupBy([\"site_id\",\"month\"]).\n",
    "            agg(F.mean(F.col(\"avg_air_temperature_impute\")).alias(\"temp\")).\n",
    "            withColumns({\n",
    "                \"highest\": F.row_number().over(window_asce),\n",
    "                \"lowest\":  F.row_number().over(window_desc)\n",
    "            }).\n",
    "            withColumn(\n",
    "                colName = \"is_peak\",\n",
    "                col     = F.when(condition=(F.col(\"highest\") <= 3) | (F.col(\"lowest\") <= 3), value=1).\n",
    "                            otherwise(value=0)\n",
    "            ).\n",
    "            drop(\"highest\").\n",
    "            drop(\"lowest\").\n",
    "            drop(\"temp\")\n",
    "        )\n",
    "\n",
    "        ## ------------------------------------------------------------------------------------- Initialization Model\n",
    "        return PeakMonthExtractorModel(isPeakDF=_is_peak_DF)\n",
    "\n",
    "## ============================================================================================= Pipeline List Update\n",
    "peak_month_extractor = PeakMonthExtractor()\n",
    "pipeline_list03 = pipeline_list02 + [peak_month_extractor]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **2.6.1.6 Bucketizer**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ============================================================================================= Implementation\n",
    "## --------------------------------------------------------------------------------------------- Params definition\n",
    "input_cols_bucket  = \"avg_cloud_coverage_impute\"\n",
    "output_cols_bucket = f\"{input_cols_bucket}_bucket\"\n",
    "## ============================================================================================= Pipeline List Update\n",
    "bucketer = Bucketizer(\n",
    "    inputCol  = input_cols_bucket,\n",
    "    outputCol = output_cols_bucket,\n",
    "    splits    = quantile_cloud_coverage\n",
    ")\n",
    "pipeline_list04 = pipeline_list03 + [bucketer]"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **2.6.1.7 Main Pipeline**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data_transformation_model_weather = Pipeline(stages = pipeline_list04).fit(dataset = DF_weather)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **2.6.1.8 Transformation and Cleanning**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "DF_weather_final =  (\n",
    "    data_transformation_model_weather.transform(DF_weather_stream).\n",
    "    drop(\"avg_cloud_coverage_impute\")\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **2.6.2 Building Metadata**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **2.6.2.1 Class Combination**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ============================================================================================= Implementation\n",
    "## --------------------------------------------------------------------------------------------- Transformer Definition\n",
    "def class_combination(targetDF:DataFrame):\n",
    "    return (\n",
    "        targetDF.\n",
    "        withColumns({\n",
    "            ## Feature creation and transformation\n",
    "            \"primary_use_modify\": F.when((F.col(\"primary_use\") == \"Warehouse\") |\n",
    "                                         (F.col(\"primary_use\") == \"Services\") |\n",
    "                                         (F.col(\"primary_use\") == \"Parking\"),\n",
    "                                         \"Education\")\\\n",
    "                                   .when((F.col(\"primary_use\") != \"Community\") &\n",
    "                                         (F.col(\"primary_use\") != \"Entertainment\") &\n",
    "                                         (F.col(\"primary_use\") != \"Residential\") &\n",
    "                                         (F.col(\"primary_use\") != \"Office\") &\n",
    "                                         (F.col(\"primary_use\") != \"Education\") &\n",
    "                                         (F.col(\"primary_use\") != \"Warehouse\") &\n",
    "                                         (F.col(\"primary_use\") != \"Services\") &\n",
    "                                         (F.col(\"primary_use\") != \"Parking\"),\n",
    "                                         \"Residential\")\\\n",
    "                                    .otherwise(F.col(\"primary_use\")),\n",
    "        }).\n",
    "        drop(\"primary_use\")\n",
    "    )\n",
    "\n",
    "## --------------------------------------------------------------------------------------------- Transforming\n",
    "DF_building02 = class_combination(targetDF=DF_building)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **2.6.2.2 Bucketizing**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ============================================================================================= Implementation\n",
    "## --------------------------------------------------------------------------------------------- Params definition\n",
    "# floor_count\n",
    "floor_count_bucket = [-float(\"inf\"),1.1,float(\"inf\")]\n",
    "\n",
    "# latent_s\n",
    "latent_s_bucket = [-float(\"inf\"),4.5,float(\"inf\")]\n",
    "\n",
    "# latent_y\n",
    "latent_y_bucket = quantile_bucket_splits(\n",
    "    targetDF    = DF_building,\n",
    "    targetCol   = \"latent_y\",\n",
    "    bucketRange = (35,None)\n",
    ")\n",
    "\n",
    "# Bucketizing params\n",
    "input_cols_bucket  = [\"floor_count\",\"latent_s\",\"latent_y\"]\n",
    "output_cols_bucket = [f\"{_col}_bucket\" for _col in input_cols_bucket]\n",
    "splits = [floor_count_bucket,latent_s_bucket,latent_y_bucket]\n",
    "## --------------------------------------------------------------------------------------------- Bucketizing\n",
    "bucketer_building = Bucketizer(\n",
    "    inputCols   = input_cols_bucket,\n",
    "    outputCols  = output_cols_bucket,\n",
    "    splitsArray = splits\n",
    ")\n",
    "DF_building_final = (\n",
    "    bucketer_building.transform(dataset=DF_building02).\n",
    "    withColumns({\n",
    "        \"is_square_feet_4e05_6e05\": F.when(\n",
    "            condition=(F.col(\"square_feet\")>4e05) & (F.col(\"square_feet\")<=6e05),\n",
    "            value=1\n",
    "        ).otherwise(value=0),\n",
    "        \"is_square_feet_6e05_inf\": F.when(\n",
    "            condition=(F.col(\"square_feet\")>6e05),\n",
    "            value=1\n",
    "        ).otherwise(value=0),\n",
    "        \"square_feet_bucket\": F.when(\n",
    "            condition=F.col(\"square_feet\")>4e05,\n",
    "            value=4e05\n",
    "        ).otherwise(value=F.col(\"square_feet\"))\n",
    "    }).\n",
    "    drop(\"year_use\").\n",
    "    drop(\"latent_s\").\n",
    "    drop(\"latent_y\").\n",
    "    drop(\"square_feet\").\n",
    "    drop(\"floor_count\").\n",
    "    drop(\"avg_cloud_coverage_impute\").\n",
    "    drop(\"floor_count\").\n",
    "    drop(\"latent_s\").\n",
    "    drop(\"latent_y\").\n",
    "    drop(\"row_id\")\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **2.6.3 Joining and Further Transformation**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ============================================================================================= Implementation\n",
    "## --------------------------------------------------------------------------------------------- Joining\n",
    "DF_join = DF_weather_final.join(\n",
    "    other = DF_building_final,\n",
    "    on    = \"site_id\",\n",
    "    how   = \"inner\"\n",
    ")\n",
    "## --------------------------------------------------------------------------------------------- Final Features Extraction\n",
    "DF_final = (\n",
    "    DF_join.\n",
    "    withColumns({\n",
    "        \"year_use\": F.col(\"year\") - F.col(\"year_built\"),\n",
    "        \"year_use_bucket\": F.when(F.col(\"year_use\") > 57, 58).otherwise(F.col(\"year_use\"))\n",
    "    }).\n",
    "    drop(\"year\").\n",
    "    drop(\"year_use\").\n",
    "    drop(\"year_built\")\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "---"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **2.7 Prediction and Aggregation**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **2.7.1 Loading Pipeline And Model**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ============================================================================================= Implementation\n",
    "## --------------------------------------------------------------------------------------------- Evaluator\n",
    "class RMSLE(Evaluator, DefaultParamsReadable, DefaultParamsWritable):\n",
    "\n",
    "    def __init__(self, predictionCol:str=\"value_pred\", labelCol:str=\"value\") -> None:\n",
    "        super(RMSLE,self).__init__()\n",
    "        self.predictionCol = predictionCol\n",
    "        self.labelCol = labelCol\n",
    "\n",
    "    def _evaluate(self, dataset) -> float:\n",
    "        \"\"\"\n",
    "        Define the RMSLE metrics.\n",
    "        :param dataset: DataStorage contain labels and predictions (type: pyspark.sql.DataFrame)\n",
    "        :return: Metrics (type: float)\n",
    "        \"\"\"\n",
    "        return (\n",
    "            dataset.\n",
    "            select(\n",
    "                # RMSLE =\n",
    "                # sqrt(mean((log(y_pred + 1) - log(y_true + 1))^2))\n",
    "                F.sqrt(\n",
    "                    F.mean(\n",
    "                        F.power(\n",
    "                            F.log(F.col(self.labelCol) + 1) - F.log(F.col(self.predictionCol) + 1),\n",
    "                            F.lit(2)\n",
    "                ))).alias(\"RMSLE\")\n",
    "            # .collect will return a list which looks like,\n",
    "            # [ROW(...)] and [0][0] means the first element in the first column of the first row\n",
    "            ).collect()[0][0]\n",
    "        )\n",
    "\n",
    "    def isLargerBetter(self) -> bool:\n",
    "        \"\"\"\n",
    "        Is the metrics larger is good.\n",
    "        :return: False\n",
    "        \"\"\"\n",
    "        return False\n",
    "## --------------------------------------------------------------------------------------------- Pipeline Model\n",
    "model = PipelineModel.load(\"../fine_tuning_step_one\")\n",
    "## --------------------------------------------------------------------------------------------- Prediction\n",
    "DF_pred_final = (\n",
    "    model.transform(dataset = DF_final)\n",
    "    .withColumns({\n",
    "        \"start_weather_ts\": F.col(\"window.start\").cast(\"int\"),\n",
    "        \"end_weather_ts\": F.col(\"window.end\").cast(\"int\"),\n",
    "        \"value_pred_adjust\": F.when(F.col(\"value_pred\") < 0, 0).otherwise(F.col(\"value_pred\"))\n",
    "    })\n",
    ")\n",
    "## --------------------------------------------------------------------------------------------- Stream manager class\n",
    "class QueryStream:\n",
    "    def __init__(self, targetStreamDF:DataFrame, sparkSession:SparkSession):\n",
    "        self._check_point_path = None\n",
    "        self._parquet_path = None\n",
    "        self._trigger_time = None\n",
    "        self._DF = targetStreamDF\n",
    "        self._is_called = False\n",
    "        self._is_stream = False\n",
    "        self._is_memory:bool = False\n",
    "        self._is_parquet:bool = False\n",
    "        self._query = None\n",
    "        self._spark_session = sparkSession\n",
    "\n",
    "    def _create_stream(self) -> None:\n",
    "        \"\"\" Internal function to create a stream\"\"\"\n",
    "        if not self._is_called:\n",
    "            raise ValueError(\"Do not call internal function directly\")\n",
    "\n",
    "        if self._is_memory:\n",
    "            # Using memory\n",
    "            print(\"Using memory mode\")\n",
    "            self._query = (\n",
    "                self._DF\n",
    "                .writeStream\n",
    "                .outputMode(\"append\")\n",
    "                # Why not use foreachBatch:\n",
    "                #   For minibatch mode,\n",
    "                #   there could be a situation that spark process\n",
    "                #   a single batch for multiple times,\n",
    "                #   foreachBatch can only guarantee each batch\n",
    "                #   being process 'at least once',\n",
    "                #   however, could not guarantee exactly once,\n",
    "                #   The original query mode can guarantee exactly once,\n",
    "                # There is a way to implement exactly once when using foreachBatch,\n",
    "                # using batch ID to implement roll back or skip process manually\n",
    "                .format(\"memory\")\n",
    "                # .foreachBatch(for_each_batch_function)\n",
    "                .trigger(processingTime=f\"{self._trigger_time} seconds\")\n",
    "                # The different watermark strategy under each mode:\n",
    "                #   1. Append: Only output the data when the window event-time are all later than watermark,\n",
    "                #   2. Update: Output the data once the process is done, will update processed batch when new data arrive,\n",
    "                #              will delete the status when the event time for a batch is later than watermark threshold\n",
    "                #   3. Complete: Will not delete the status, and will process the data everytime new data arrive,\n",
    "                #                or to say, watermark is a little bit useless for this mode\n",
    "                .queryName(\"test\")\n",
    "            )\n",
    "        elif self._is_parquet:\n",
    "            # Using parquet\n",
    "            print(\"Using parquet mode\")\n",
    "            self._query = (\n",
    "                self._DF\n",
    "                .writeStream\n",
    "                .option(\"checkpointLocation\", self._check_point_path)\n",
    "                .format(\"parquet\")\n",
    "                .trigger(processingTime=f\"{self._trigger_time} seconds\")\n",
    "                .outputMode(\"append\")\n",
    "                .option(\"path\", self._parquet_path)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Please set a single mode\")\n",
    "\n",
    "    def start_stream(\n",
    "            self,\n",
    "            isMemory:bool  = None,\n",
    "            isParquet:bool = None,\n",
    "            triggerTime:int|str  = 7,\n",
    "            parquetPath:str|None = None,\n",
    "            checkPointPath:str|None = None\n",
    "    ):\n",
    "        \"\"\" Function to start the stream\"\"\"\n",
    "        try:\n",
    "            # Mode\n",
    "            if self._is_stream:\n",
    "                raise ValueError(\"The stream is on, please stop first.\")\n",
    "            else:\n",
    "                if self._query is None:\n",
    "                    if isMemory and isParquet:\n",
    "                        raise ValueError(\"Please select a single mode\")\n",
    "                    if not isMemory and not isParquet and not self._is_memory and not self._is_parquet:\n",
    "                        raise ValueError(\"Please select at least one mode\")\n",
    "                    if isMemory:\n",
    "                        self._is_memory = isMemory\n",
    "                    else:\n",
    "                        self._is_parquet = isParquet\n",
    "\n",
    "                    # Trigger time\n",
    "                    triggerTime = int(triggerTime)\n",
    "                    if isinstance(triggerTime, int) and triggerTime > 0:\n",
    "                        # Trigger time should be a positive integer\n",
    "                        self._trigger_time = str(triggerTime)\n",
    "\n",
    "                    # Parquet path\n",
    "                    if isinstance(parquetPath, str):\n",
    "                        self._parquet_path = parquetPath\n",
    "                    elif parquetPath is None and self._parquet_path is None:\n",
    "                        self._parquet_path = \"../parquet/weather\"\n",
    "\n",
    "                    # Checkpoint path\n",
    "                    if isinstance(checkPointPath, str):\n",
    "                        self._check_point_path = parquetPath\n",
    "                    elif checkPointPath is None and self._check_point_path is None:\n",
    "                        self._check_point_path = \"../checkpoint\"\n",
    "\n",
    "                    # Create a query\n",
    "                    self._is_called = True\n",
    "                    self._create_stream()\n",
    "                    self._is_stream = True\n",
    "                    self._is_called = False\n",
    "                self._query = self._query.start()\n",
    "                return self._query\n",
    "        except ValueError as e:\n",
    "            print(\"Error when open the stream:\")\n",
    "            print(\"---------------------------\")\n",
    "            print(str(e))\n",
    "\n",
    "    def end_stream(self) -> None:\n",
    "        \"\"\" End stream \"\"\"\n",
    "        try:\n",
    "            if self._is_stream:\n",
    "                self._query.stop()\n",
    "                self._query = None\n",
    "                self._is_stream = False\n",
    "                print(\"Stream has been stop\")\n",
    "            else:\n",
    "                raise ValueError(\"The stream has been already stopped\")\n",
    "        except ValueError as e:\n",
    "            print(\"Error when end the stream:\")\n",
    "            print(\"--------------------------\")\n",
    "            print(str(e))\n",
    "\n",
    "    def get_data(self) -> DataFrame:\n",
    "        \"\"\" Get the current data \"\"\"\n",
    "        try:\n",
    "            if not self._is_memory:\n",
    "                raise ValueError(\"You could only get the data in memory mode\")\n",
    "            return self._spark_session.sql(\"SELECT * FROM test\")\n",
    "        except ValueError as e:\n",
    "            print(\"Error when get the data:\")\n",
    "            print(\"------------------------\")\n",
    "            print(str(e))"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **2.7.2 Debug**"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T10:35:17.261092Z",
     "start_time": "2025-10-27T10:35:17.101717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## ============================================================================================= Explanation\n",
    "# This part is for debug and help me understand the input rate for each turn\n",
    "## ============================================================================================= Implementation\n",
    "## --------------------------------------------------------------------------------------------- Query Creation\n",
    "trigger_time = 5\n",
    "stream_manager = QueryStream(targetStreamDF=DF_pred_final, sparkSession=spark_session)\n",
    "query = stream_manager.start_stream(isMemory=True, triggerTime=trigger_time)\n",
    "current_end = None\n",
    "start = False\n",
    "i = 0\n",
    "## --------------------------------------------------------------------------------------------- Streaming Output\n",
    "try:\n",
    "    while start:\n",
    "        print(\"waiting...\")\n",
    "        time.sleep(trigger_time)\n",
    "        print(\"sending...\")\n",
    "        progress = query.lastProgress[\"sources\"]\n",
    "        if progress:\n",
    "            print(\"-\"*50)\n",
    "            print(f\"Turn {i}\")\n",
    "            print(\n",
    "                f\"\\tstartOffset:            {progress[0]['startOffset']}\\n\"\n",
    "                f\"\\tendOffset:              {progress[0]['endOffset']}\\n\"\n",
    "                f\"\\tnumInputRows:           {progress[0]['numInputRows']}\\n\"\n",
    "                f\"\\tinputRowsPerSecond:     {progress[0]['inputRowsPerSecond']}\\n\"\n",
    "                f\"\\tprocessedRowsPerSecond: {progress[0]['processedRowsPerSecond']}\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"\\tNo progress\")\n",
    "        i += 1\n",
    "except Exception as e:\n",
    "    print(\"Error when reading DF:\")\n",
    "    print(str(e))\n",
    "finally:\n",
    "    stream_manager.end_stream()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using memory mode\n",
      "Stream has been stop\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **2.7.3 Aggregation**"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **2.7.3.1 Hourly Load Forecasting for Energy Consumption**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ============================================================================================= Implementation\n",
    "## --------------------------------------------------------------------------------------------- DataFrame Definition\n",
    "DF_prediction = (\n",
    "    DF_pred_final\n",
    "    .select(\"window\",\"month\",\"day\",\"site_id\",\"building_id\",\"start_hour\",\"value_pred_adjust\")\n",
    ")\n",
    "## --------------------------------------------------------------------------------------------- Query Creation\n",
    "trigger_time = 5\n",
    "stream_manager = QueryStream(targetStreamDF=DF_prediction, sparkSession=spark_session)\n",
    "query = stream_manager.start_stream(isMemory=True, triggerTime=trigger_time)\n",
    "current_end = None\n",
    "i = 0\n",
    "start = True\n",
    "## --------------------------------------------------------------------------------------------- Streaming Output\n",
    "try:\n",
    "    while True:\n",
    "        print(\"waiting...\")\n",
    "        time.sleep(trigger_time)\n",
    "        print(\"sending...\")\n",
    "        progress = query.lastProgress[\"sources\"]\n",
    "        if progress and start:\n",
    "            # Debug message\n",
    "            print(\"=\"*50)\n",
    "            print(f\"Turn {i}\")\n",
    "            print(\"Debug\" + \"-\" * 40)\n",
    "            print(\n",
    "                f\"\\tstartOffset:            {progress[0]['startOffset']}\\n\"\n",
    "                f\"\\tendOffset:              {progress[0]['endOffset']}\\n\"\n",
    "                f\"\\tnumInputRows:           {progress[0]['numInputRows']}\\n\"\n",
    "                f\"\\tinputRowsPerSecond:     {progress[0]['inputRowsPerSecond']}\\n\"\n",
    "                f\"\\tprocessedRowsPerSecond: {progress[0]['processedRowsPerSecond']}\"\n",
    "            )\n",
    "            print(\"Result\" + \"-\" * 41)\n",
    "            print(\"First 5\")\n",
    "            print(\n",
    "                stream_manager\n",
    "                .get_data()\n",
    "                .orderBy(\n",
    "                    F.col(\"window.start\"),F.col(\"site_id\"),\n",
    "                    F.col(\"building_id\"), F.col(\"start_hour\")\n",
    "                )\n",
    "                .select(\"window.start\",\"site_id\",\"building_id\",\"start_hour\",\"value_pred_adjust\")\n",
    "                .show(5)\n",
    "            )\n",
    "            print(\"Last 5\")\n",
    "            print(\n",
    "                stream_manager\n",
    "                .get_data()\n",
    "                .orderBy(\n",
    "                    F.col(\"window.start\").desc(),F.col(\"site_id\").desc(),\n",
    "                    F.col(\"building_id\").desc(), F.col(\"start_hour\").desc()\n",
    "                )\n",
    "                .select(\"window.start\",\"site_id\",\"building_id\",\"start_hour\",\"value_pred_adjust\")\n",
    "                .show(5)\n",
    "            )\n",
    "        else:\n",
    "            print(f\"\\tNo progress\")\n",
    "        i += 1\n",
    "except Exception as e:\n",
    "    print(\"Error when reading DF:\")\n",
    "    print(str(e))\n",
    "finally:\n",
    "    stream_manager.end_stream()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **2.7.3.2 Weekly Load Forecasting for Energy Consumption**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ============================================================================================= Implementation\n",
    "## --------------------------------------------------------------------------------------------- DataFrame Definition\n",
    "DF_weekly = (\n",
    "    DF_pred_final\n",
    "    .groupBy(\"window\",\"building_id\",\"start_hour\")\n",
    "    .agg(F.sum(\"value_pred_adjust\").alias(\"total_value_pred_adjust\"))\n",
    ")\n",
    "## --------------------------------------------------------------------------------------------- Query Creation\n",
    "trigger_time = 7\n",
    "stream_manager = QueryStream(targetStreamDF=DF_weekly, sparkSession=spark_session)\n",
    "query = stream_manager.start_stream(isMemory=True, triggerTime=trigger_time)\n",
    "current_end = None\n",
    "i = 0\n",
    "start = True\n",
    "## --------------------------------------------------------------------------------------------- Streaming Output\n",
    "try:\n",
    "    while True:\n",
    "        print(\"waiting...\")\n",
    "        time.sleep(trigger_time)\n",
    "        print(\"sending...\")\n",
    "        progress = query.lastProgress[\"sources\"]\n",
    "        if progress and start:\n",
    "            # Debug message\n",
    "            print(\"=\"*50)\n",
    "            print(f\"Turn {i}\")\n",
    "            print(\"Debug\" + \"-\" * 40)\n",
    "            print(\n",
    "                f\"\\tstartOffset:            {progress[0]['startOffset']}\\n\"\n",
    "                f\"\\tendOffset:              {progress[0]['endOffset']}\\n\"\n",
    "                f\"\\tnumInputRows:           {progress[0]['numInputRows']}\\n\"\n",
    "                f\"\\tinputRowsPerSecond:     {progress[0]['inputRowsPerSecond']}\\n\"\n",
    "                f\"\\tprocessedRowsPerSecond: {progress[0]['processedRowsPerSecond']}\"\n",
    "            )\n",
    "            print(\"Result\" + \"-\" * 41)\n",
    "            print(\"First 5 weeks\")\n",
    "\n",
    "            print(\n",
    "                stream_manager\n",
    "                .get_data()\n",
    "                .orderBy(F.col(\"window.start\"),F.col(\"building_id\"),F.col(\"start_hour\"))\n",
    "                .select(F.col(\"window.start\"),\"building_id\",\"start_hour\",\"total_value_pred_adjust\")\n",
    "                .show(5)\n",
    "            )\n",
    "            print(\"Last 5 weeks\")\n",
    "            print(\n",
    "                stream_manager\n",
    "                .get_data()\n",
    "                .orderBy(F.col(\"window.start\").desc(),F.col(\"building_id\").desc(),F.col(\"start_hour\"))\n",
    "                .select(F.col(\"window.start\"),\"building_id\",\"start_hour\",\"total_value_pred_adjust\")\n",
    "                .show(5)\n",
    "            )\n",
    "        else:\n",
    "            print(f\"\\tNo progress\")\n",
    "        i += 1\n",
    "except Exception as e:\n",
    "    print(\"Error when reading DF:\")\n",
    "    print(str(e))\n",
    "finally:\n",
    "    stream_manager.end_stream()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### **2.7.3.3 Daily Load Forecasting for Energy Consumption**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ============================================================================================= Implementation\n",
    "DF_daily = (\n",
    "    DF_pred_final\n",
    "    .groupBy(F.window(\"window\",\"14 second\"),\"site_id\",\"start_hour\")\n",
    "    .agg(F.mean(\"value_pred_adjust\").alias(\"mean_value_pred_adjust\"))\n",
    "    .groupBy(\"window\",\"site_id\")\n",
    "    .agg(F.mean(\"mean_value_pred_adjust\").alias(\"daily_value_pred_adjust\"))\n",
    ")\n",
    "## --------------------------------------------------------------------------------------------- Query Creation\n",
    "trigger_time = 14\n",
    "stream_manager = QueryStream(targetStreamDF=DF_daily, sparkSession=spark_session)\n",
    "query = stream_manager.start_stream(isMemory=True, triggerTime=trigger_time)\n",
    "current_end = None\n",
    "i = 0\n",
    "start = True\n",
    "## --------------------------------------------------------------------------------------------- Streaming Output\n",
    "try:\n",
    "    while True:\n",
    "        print(\"waiting...\")\n",
    "        time.sleep(trigger_time)\n",
    "        print(\"sending...\")\n",
    "        progress = query.lastProgress[\"sources\"]\n",
    "        if progress and start:\n",
    "            # Debug message\n",
    "            print(\"=\"*50)\n",
    "            print(f\"Turn {i}\")\n",
    "            print(\"Debug\" + \"-\" * 40)\n",
    "            print(\n",
    "                f\"\\tstartOffset:            {progress[0]['startOffset']}\\n\"\n",
    "                f\"\\tendOffset:              {progress[0]['endOffset']}\\n\"\n",
    "                f\"\\tnumInputRows:           {progress[0]['numInputRows']}\\n\"\n",
    "                f\"\\tinputRowsPerSecond:     {progress[0]['inputRowsPerSecond']}\\n\"\n",
    "                f\"\\tprocessedRowsPerSecond: {progress[0]['processedRowsPerSecond']}\"\n",
    "            )\n",
    "            print(\"Result\" + \"-\" * 41)\n",
    "            print(\n",
    "                stream_manager\n",
    "                .get_data()\n",
    "                .orderBy(\"window.start\",\"site_id\")\n",
    "                .select(\n",
    "                    F.col(\"window.start\").cast(\"int\").alias(\"start\"),\n",
    "                    F.col(\"window.end\").cast(\"int\").alias(\"end\"),\n",
    "                    \"site_id\",\"daily_value_pred_adjust\"\n",
    "                )\n",
    "                .show(5)\n",
    "            )\n",
    "        else:\n",
    "            print(f\"\\tNo progress\")\n",
    "        i += 1\n",
    "except Exception as e:\n",
    "    print(\"Error when reading DF:\")\n",
    "    print(str(e))\n",
    "finally:\n",
    "    stream_manager.end_stream()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### **2.7.4 Data Persistence**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ============================================================================================= Definition\n",
    "DF_prediction = (\n",
    "    DF_pred_final\n",
    "    .select(\"window\",\"month\",\"day\",\"site_id\",\"building_id\",\"start_hour\",\"value_pred_adjust\")\n",
    ")\n",
    "DF_weekly = (\n",
    "    DF_pred_final\n",
    "    .groupBy(\"window\",\"building_id\",\"start_hour\")\n",
    "    .agg(F.sum(\"value_pred_adjust\").alias(\"total_value_pred_adjust\"))\n",
    ")\n",
    "DF_daily = (\n",
    "    DF_pred_final\n",
    "    .groupBy(F.window(\"window\",\"14 second\"),\"site_id\",\"start_hour\")\n",
    "    .agg(F.mean(\"value_pred_adjust\").alias(\"mean_value_pred_adjust\"))\n",
    "    .groupBy(\"window\",\"site_id\")\n",
    "    .agg(F.mean(\"mean_value_pred_adjust\").alias(\"daily_value_pred_adjust\"))\n",
    ")\n",
    "## ============================================================================================= Start Stream\n",
    "## --------------------------------------------------------------------------------------------- Hourly\n",
    "stream_manager01 = QueryStream(\n",
    "    targetStreamDF = DF_prediction,\n",
    "    sparkSession   = spark_session,\n",
    ")\n",
    "_ = stream_manager01.start_stream(isParquet=True, parquetPath=\"../DataStorage/EnergyConsumption/Prediction/hourly\", checkPointPath=\"../CheckPoint/writing/hourly\")\n",
    "## --------------------------------------------------------------------------------------------- Weekly\n",
    "stream_manager02 = QueryStream(\n",
    "    targetStreamDF = DF_weekly,\n",
    "    sparkSession   = spark_session,\n",
    ")\n",
    "_ = stream_manager02.start_stream(isParquet=True, parquetPath=\"../DataStorage/EnergyConsumption/Prediction/weekly\", checkPointPath=\"../CheckPoint/writing/weekly\")\n",
    "## --------------------------------------------------------------------------------------------- Daily\n",
    "stream_manager03 = QueryStream(\n",
    "    targetStreamDF = DF_daily,\n",
    "    sparkSession   = spark_session,\n",
    ")\n",
    "_ = stream_manager03.start_stream(isParquet=True, parquetPath=\"../DataStorage/EnergyConsumption/Prediction/daily\", checkPointPath=\"../CheckPoint/writing/daily\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ============================================================================================= End Stream\n",
    "stream_manager01.end_stream()\n",
    "stream_manager02.end_stream()\n",
    "stream_manager03.end_stream()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### **2.7.5 Dashboard Producer**"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ============================================================================================= Schema Definition\n",
    "## --------------------------------------------------------------------------------------------- Daily\n",
    "daily_schema = StructType([\n",
    "    StructField(\"window\", StructType([\n",
    "            StructField(\"start\",TimestampType()),\n",
    "            StructField(\"end\"  ,TimestampType())\n",
    "        ])),\n",
    "    StructField(\"site_id\", IntegerType()),\n",
    "    StructField(\"daily_value_pred_adjust\", DoubleType())\n",
    "])\n",
    "## --------------------------------------------------------------------------------------------- Weekly\n",
    "weekly_schema = StructType([\n",
    "    StructField(\"window\", StructType([\n",
    "        StructField(\"start\", TimestampType()),\n",
    "        StructField(\"end\",   TimestampType())\n",
    "    ])),\n",
    "    StructField(\"building_id\",             IntegerType()),\n",
    "    StructField(\"start_hour\",              IntegerType()),\n",
    "    StructField(\"total_value_pred_adjust\", DoubleType())\n",
    "])\n",
    "## --------------------------------------------------------------------------------------------- Hourly\n",
    "prediction_schema = StructType([\n",
    "    StructField(\"window\", StructType([\n",
    "        StructField(\"start\", TimestampType()),\n",
    "        StructField(\"end\",   TimestampType())\n",
    "    ])),\n",
    "    StructField(\"site_id\",             IntegerType()),\n",
    "    StructField(\"building_id\",         IntegerType()),\n",
    "    StructField(\"start_hour\",          IntegerType()),\n",
    "    StructField(\"day\",                 IntegerType()),\n",
    "    StructField(\"month\",               IntegerType()),\n",
    "    StructField(\"value_pred_adjust\",   DoubleType())\n",
    "])\n",
    "## ============================================================================================= Loading\n",
    "## --------------------------------------------------------------------------------------------- Function\n",
    "def get_stream_parquet(\n",
    "        folderName:str,\n",
    "        schema:StructType,\n",
    "        sparkSession:SparkSession\n",
    ") -> DataFrame:\n",
    "    \"\"\" Get the stream DataFrame \"\"\"\n",
    "    return (\n",
    "        sparkSession\n",
    "        .readStream\n",
    "        .format(\"parquet\")\n",
    "        .schema(schema)\n",
    "        .load(\"../DataStorage/EnergyConsumption/Prediction/\" + folderName)\n",
    "    )\n",
    "## --------------------------------------------------------------------------------------------- Daily\n",
    "daily_stream = get_stream_parquet(\n",
    "    folderName = \"daily/\",\n",
    "    schema     = daily_schema,\n",
    "    sparkSession = spark_session\n",
    ")\n",
    "## --------------------------------------------------------------------------------------------- Weekly\n",
    "weekly_stream = get_stream_parquet(\n",
    "    folderName = \"weekly/\",\n",
    "    schema     = weekly_schema,\n",
    "    sparkSession = spark_session\n",
    ")\n",
    "## --------------------------------------------------------------------------------------------- Hourly\n",
    "prediction_stream = get_stream_parquet(\n",
    "    folderName = \"hourly/\",\n",
    "    schema     = prediction_schema,\n",
    "    sparkSession = spark_session\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ============================================================================================= Sender Definition\n",
    "## --------------------------------------------------------------------------------------------- Transformation Function\n",
    "def get_sending_stream(streamDF:DataFrame, targetKey:str) -> DataFrame:\n",
    "    \"\"\" Cast DataFrame to string \"\"\"\n",
    "    return (\n",
    "        streamDF\n",
    "        .selectExpr(\"to_json(struct(*)) AS value\")\n",
    "        .writeStream\n",
    "        .format(\"kafka\")\n",
    "        .option(\"kafka.bootstrap.servers\", \"kafka:9092\")\n",
    "        .option(\"topic\", f\"prediction_{targetKey}\")\n",
    "        .option(\"checkpointLocation\", \"../CheckPoint/loading/\" + targetKey)\n",
    "        .outputMode(\"append\")\n",
    "    )\n",
    "## --------------------------------------------------------------------------------------------- Hourly\n",
    "prediction_query = get_sending_stream(streamDF=prediction_stream,targetKey=\"hourly\")\n",
    "## --------------------------------------------------------------------------------------------- Weekly\n",
    "weekly_query = get_sending_stream(streamDF=weekly_stream,targetKey=\"weekly\")\n",
    "## --------------------------------------------------------------------------------------------- Daily\n",
    "daily_query = get_sending_stream(streamDF=daily_stream,targetKey=\"daily\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ============================================================================================= Sending\n",
    "## --------------------------------------------------------------------------------------------- Hourly\n",
    "prediction_query_start = prediction_query.start()\n",
    "## --------------------------------------------------------------------------------------------- Weekly\n",
    "weekly_query_start = weekly_query.start()\n",
    "## --------------------------------------------------------------------------------------------- Daily\n",
    "daily_query_start = daily_query.start()"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## ============================================================================================= Stopping\n",
    "## --------------------------------------------------------------------------------------------- Hourly\n",
    "prediction_query_start.stop()\n",
    "## --------------------------------------------------------------------------------------------- Weekly\n",
    "weekly_query_start.stop()\n",
    "## --------------------------------------------------------------------------------------------- Daily\n",
    "daily_query_start.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
